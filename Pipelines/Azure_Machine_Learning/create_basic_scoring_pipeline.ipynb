{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic scoring pipeline #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks is just an addition to `create_basic_training_pipeline.ipynb` to demonstrate how to use the trained model to make scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "from msrest.exceptions import HttpOperationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are going to list all files and send them to the Form Recognizer one by one, we will need couple more parameters compare to training pipeline:\n",
    "\n",
    "- storage_name: a storage name that contains input data\n",
    "- storage_key: a storage key to get access to the storage with input data\n",
    "- container_name: the name of the container that contains folder with input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<provide it here>\"\n",
    "wrksp_name = \"<provide it here>\"\n",
    "resource_group = \"<provide it here>\"\n",
    "compute_name = \"mycluster\"\n",
    "min_nodes = 0\n",
    "max_nodes = 4\n",
    "vm_priority = \"lowpriority\"\n",
    "vm_size = \"Standard_F2s_v2\"\n",
    "project_folder = \"basic_scoring_steps\"\n",
    "fr_endpoint = \"<provide it here>\"\n",
    "fr_key = \"<provide it here>\"\n",
    "storage_name = \"<provide it here>\"\n",
    "storage_key = \"<provide it here>\"\n",
    "container_name = \"data\"\n",
    "datastore_name = \"data_ds\"\n",
    "scoring_ds_name = \"basic_scoring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a reference to the workspace. If it doesn't exist there is no sense to create new one because we don't have any models anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aml_workspace = Workspace.get(\n",
    "        name=wrksp_name,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group=resource_group)\n",
    "    print(\"Found the existing Workspace\")\n",
    "except Exception as e:\n",
    "    print(f\"Workspace doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a compute cluster for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_name in aml_workspace.compute_targets:\n",
    "    compute_target = aml_workspace.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f\"Found existing compute target {compute_name} so using it\")\n",
    "else:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_size,\n",
    "        vm_priority=vm_priority,\n",
    "        min_nodes=min_nodes,\n",
    "        max_nodes=max_nodes,\n",
    "    )\n",
    "\n",
    "    compute_target = ComputeTarget.create(aml_workspace, compute_name,\n",
    "                                                  compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to mount blob storage with input data tp scoring compute cluster. To do that we need to register blob container as a data store in AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    blob_datastore = Datastore.get(aml_workspace, datastore_name)\n",
    "    print(\"Found Blob Datastore with name: %s\" % datastore_name)\n",
    "except HttpOperationError:\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=aml_workspace,\n",
    "        datastore_name=datastore_name,\n",
    "        account_name=storage_name,\n",
    "        container_name=container_name,\n",
    "        account_key=storage_key)\n",
    "print(\"Registered blob datastore with name: %s\" % datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a dataset with all input files there (it doesn't make much sense for this example, but it's very useful for parallel step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_file_path = blob_datastore.path(\"Test\")\n",
    "scoring_file_dataset = Dataset.File.from_files(path=scoring_file_path, validate=True)\n",
    "scoring_file_dataset = scoring_file_dataset.register(\n",
    "    aml_workspace, scoring_ds_name, create_new_version=True)\n",
    "print(\"Dataset has been registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need pipeline data object to store all outputs from scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_output = PipelineData(\n",
    "    \"scoring_output\",\n",
    "    datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just one step here: scoring. We will list all files and make scoring one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_step = PythonScriptStep(\n",
    "    name = \"scoring\",\n",
    "    script_name=\"score.py\",\n",
    "    inputs=[scoring_file_dataset.as_named_input(\"scoring_files\")],\n",
    "    outputs=[scoring_output],\n",
    "    arguments=[\n",
    "        \"--output\", scoring_output,\n",
    "        \"--fr_endpoint\", fr_endpoint,\n",
    "        \"--fr_key\", fr_key],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [scoring_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline object with one step only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=aml_workspace, steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(aml_workspace, 'scoring_basic_exp').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the pipeline as an reusable entity in AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.publish(\n",
    "    name=\"basic_scoring\",\n",
    "    description=\"Scoring data using form recognizer single model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coursera)",
   "language": "python",
   "name": "coursera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
