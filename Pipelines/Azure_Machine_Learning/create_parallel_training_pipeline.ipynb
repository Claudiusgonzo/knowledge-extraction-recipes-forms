{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training several models in parallel #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to show how to use Azure Machine Learning service in order to automate Form Recognizer service training. You will be able to see how to setup AML workspace, create a compute, execute a basic python script as a pipeline step, and store all metadata from Form Recognizer in AML model store. In this notebook we are going to use ParalleRunStep in order to execute several training processes in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "from msrest.exceptions import HttpOperationError\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import Environment\n",
    "from azureml.contrib.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.core import Keyvault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute this notebook, you need to provide some parameters. There are several formal categories to provide.\n",
    "\n",
    "### Azure Machine Learning Workspace parameters ###\n",
    "\n",
    "- subscription_id: subscription id where you host or going to create Azure Machine LEarning Workspace\n",
    "- wrksp_name: a name of the Azure Machine Learning Workspace\n",
    "- resource_group: a resource group name where you are going to have your AML workspace;\n",
    "\n",
    "### Form Recognizer Parameters ###\n",
    "- fr_endpoint: Form Recognizer endpoint\n",
    "- fr_key: Form Recognizer key to invoke REST API\n",
    "\n",
    "### Input data parameters ###\n",
    "- sas_uri: You need to create a container where you need to place all your data in separate folders. Each folder is a data source for a model in Form Recognizer. This parameter is a Shared Access Signature for the **container** that you can generate in Storage Explorer or from command line\n",
    "- storage_name: a storage name that contains input data\n",
    "- storage_key: a storage key to get access to the storage with input data\n",
    "- container_name: the name of the container that contains folder with input data\n",
    "\n",
    "You can leave all other parameters as is or modify some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<to provide>\"\n",
    "wrksp_name = \"<to provide>\"\n",
    "resource_group = \"<to provide>\"\n",
    "region = \"westus2\"\n",
    "compute_name = \"mycluster\"\n",
    "min_nodes = 0\n",
    "max_nodes = 4\n",
    "vm_priority = \"dedicated\"\n",
    "vm_size = \"Standard_DS2_v2\"\n",
    "project_folder = \"parallel_training_steps\"\n",
    "fr_endpoint = \"<to provide>\"\n",
    "fr_key = \"<to provide>\"\n",
    "sas_uri = \"<to provide>\"\n",
    "storage_name = \"<to provide>\"\n",
    "storage_key = \"<to provide>\"\n",
    "container_name = \"<to provide>\"\n",
    "datastore_name = \"training_ds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning we need to get a reference to Azure Machine Learning workspace. We will use this reference to create all needed entities. If the workspace doesn't exist we will create a new workspace based on provided parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aml_workspace = Workspace.get(\n",
    "        name=wrksp_name,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group=resource_group)\n",
    "    print(\"Found the existing Workspace\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating AML Workspace: {wrksp_name}\")\n",
    "    aml_workspace = Workspace.create(\n",
    "        name=wrksp_name,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group=resource_group,\n",
    "        create_resource_group=True,\n",
    "        location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to pass secure values to any pipeline step using integration with KeyVault. KeyVault has been deployed together with AML and it's available to any AML pipeline. The next two lines create or update key vault secrets with sasa uri and form recognizer key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault = aml_workspace.get_default_keyvault()\n",
    "keyvault.set_secret(name=\"frkey\", value = fr_key)\n",
    "keyvault.set_secret(name=\"sasuri\", value = sas_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have several steps in our machine learning pipeline. All temporary data we will store in the default blob storage that is associated woth AML workspace. In the cell below we are getting a reference to the blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = aml_workspace.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we need to create a compute that we are going to use to run pipeline. The compute is auto-scalable and it uses min_nodes as minimum number of nodes. If this value is 0, it means that compute will deploy a node (or several) just when it needs to run a step. In our case we are not going to use more than one node at the time, because we have two steps only and both of them are just basic Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_name in aml_workspace.compute_targets:\n",
    "    compute_target = aml_workspace.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f\"Found existing compute target {compute_name} so using it\")\n",
    "else:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_size,\n",
    "        vm_priority=vm_priority,\n",
    "        min_nodes=min_nodes,\n",
    "        max_nodes=max_nodes,\n",
    "    )\n",
    "\n",
    "    compute_target = ComputeTarget.create(aml_workspace, compute_name,\n",
    "                                                  compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is different compare to our basic pipeline. Because we have several folders in the container, we will need to list all of them. It means that we need to mount our input storage container to the compute cluster. In order to do that we need to register the blob container as a data store in Azure ML, and we can create a data reference to a specific folder after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    training_datastore = Datastore.get(aml_workspace, datastore_name)\n",
    "    print(\"Found Blob Datastore with name: %s\" % datastore_name)\n",
    "except HttpOperationError:\n",
    "    training_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=aml_workspace,\n",
    "        datastore_name=datastore_name,\n",
    "        account_name=storage_name,\n",
    "        container_name=container_name,\n",
    "        account_key=storage_key)\n",
    "print(\"Registered blob datastore with name: %s\" % datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_src = DataReference(\n",
    "    datastore=training_datastore,\n",
    "    data_reference_name=\"training_src\",\n",
    "    path_on_datastore=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are running training step in parallel, we need to list all available folders in advance to allow ParallelRunStep to split execution in batches. In order to do that we will need to add one more step to our pipeline and use it to create a pipeline data set with all folders there. \n",
    "In the cell below we are creating a pipeline data to store our dataset between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders = PipelineData(\"list_folders\", datastore=blob_datastore)\n",
    "list_folders_file_dataset = list_folders.as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can list all folders and store result using provided pipeline data folder in csv format. The step is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_step = PythonScriptStep(\n",
    "    name = \"list_folders\",\n",
    "    script_name=\"list.py\",\n",
    "    inputs=[training_src],\n",
    "    outputs=[list_folders_file_dataset],\n",
    "    arguments=[\n",
    "        \"--list_output\", list_folders_file_dataset,\n",
    "        \"--training_folder\", training_src],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can convert our csv file to a tabular dataset and use it as an input source to parallel run step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_folders_tabular = list_folders_file_dataset.parse_delimited_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need an entity to pass data from one step to another. We will use pipeline data. Every time when we run the pipeline, it will create an unique folder in our default blob and store our pipeline data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = PipelineData(\n",
    "    name=\"training_output\",\n",
    "    datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ParallelRunStep uses an environment that allows you to configure your compute nodes and install some additional components (or use your own container). We have just very basic environment because our code is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_env = Environment(name=\"batch_environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create and configure parallel step for executing traings in parallel. We will use 4 (max_nodes) nodes to train 4 models at the same time. Each node will get batches with size of 1. Finally, all results will be in training_output pipeline data folder. We will use this folder as is to register all results in AML model store as a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = ParallelRunConfig(\n",
    "    source_directory=project_folder,\n",
    "    entry_script=\"train.py\",\n",
    "    mini_batch_size=\"1\",\n",
    "    output_action=\"append_row\",\n",
    "    compute_target=compute_target,\n",
    "    environment=batch_env,\n",
    "    node_count=max_nodes,\n",
    "    process_count_per_node=1,\n",
    "    error_threshold=10)\n",
    "\n",
    "training_step = ParallelRunStep(\n",
    "    name=f\"trainfr\",\n",
    "    models=[],\n",
    "    parallel_run_config=training_config,\n",
    "    inputs=[list_folders_tabular],\n",
    "    output=training_output,\n",
    "    arguments=[\n",
    "        \"--fr_endpoint\", fr_endpoint\n",
    "    ],\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is taking output from the training step and register it in AML store. In fact, we could implement these two steps as a single step, but we wanted to show some aspects of AML (passing data between steps and multistep pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = PythonScriptStep(\n",
    "    name = \"registering\",\n",
    "    script_name=\"register.py\",\n",
    "    inputs=[training_output],\n",
    "    outputs=[],\n",
    "    arguments=[\"--input\", training_output],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a pipeline based on our three steps above. We just need to combine all the steps in an array and create Pipeline object using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [list_step, training_step, register_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=aml_workspace, steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to execute our pipeline. We use Experiment class to create a real execution and passing our pipeline as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(aml_workspace, 'train_parallel_exp').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of success, we would like to preserve pipeline to execute it later using the Azure portal, Python SDK or Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.publish(\n",
    "    name=\"parallel_training\",\n",
    "    description=\"Training form recognizer based on several data folders in parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (local)",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
