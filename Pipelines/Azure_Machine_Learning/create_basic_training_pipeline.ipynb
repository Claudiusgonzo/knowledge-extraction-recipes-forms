{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model training #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to show how to use Azure Machine Learning service in order to automate Form Recognizer service training. You will be able to see how to setup AML workspace, create a compute, execute a basic python script as a pipeline step, and store all metadata from Form Recognizer in AML model store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute this notebook, you need to provide some parameters. There are several formal categories to provide.\n",
    "\n",
    "### Azure Machine Learning Workspace parameters ###\n",
    "\n",
    "- subscription_id: subscription id where you host or going to create Azure Machine LEarning Workspace\n",
    "- wrksp_name: a name of the Azure Machine Learning Workspace\n",
    "- resource_group: a resource group name where you are going to have your AML workspace;\n",
    "\n",
    "### Form Recognizer Parameters ###\n",
    "- fr_endpoint: Form Recognizer endpoint\n",
    "- fr_key: Form Recognizer key to invoke REST API\n",
    "\n",
    "### Input data parameters ###\n",
    "- sas_uri: The sample data set that we use to train the model and test the model is available as a .zip file from [GitHub](https://go.microsoft.com/fwlink/?linkid=2090451). We assume that you extract this datasets to a blob container. This parameter is a Shared Access Signature for the **container** that you can generate in Storage Explorer or from command line\n",
    "\n",
    "You can leave all other parameters as is or modify some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<provide it here>\"\n",
    "wrksp_name = \"<provide it here>\"\n",
    "resource_group = \"<provide it here>\"\n",
    "region = \"westus2\"\n",
    "compute_name = \"mycluster\"\n",
    "min_nodes = 0\n",
    "max_nodes = 4\n",
    "vm_priority = \"lowpriority\"\n",
    "vm_size = \"Standard_F2s_v2\"\n",
    "project_folder = \"basic_training_steps\"\n",
    "fr_endpoint = \"<provide it here>\"\n",
    "fr_key = \"<provide it here>\"\n",
    "sas_uri = \"<provide it here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning we need to get a reference to Azure Machine Learning workspace. We will use this reference to create all needed entities. If the workspace doesn't exist we will create a new workspace based on provided parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aml_workspace = Workspace.get(\n",
    "        name=wrksp_name,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group=resource_group)\n",
    "    print(\"Found the existing Workspace\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating AML Workspace: {wrksp_name}\")\n",
    "    aml_workspace = Workspace.create(\n",
    "        name=wrksp_name,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group=resource_group,\n",
    "        create_resource_group=True,\n",
    "        location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have several steps in our machine learning pipeline. All temporary data we will store in the default blob storage that is associated woth AML workspace. In the cell below we are getting a reference to the blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = aml_workspace.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we need to create a compute that we are going to use to run pipeline. The compute is auto-scalable and it uses min_nodes as minimum number of nodes. If this value is 0, it means that compute will deploy a node (or several) just when it needs to run a step. In our case we are not going to use more than one node at the time, because we have two steps only and both of them are just basic Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_name in aml_workspace.compute_targets:\n",
    "    compute_target = aml_workspace.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f\"Found existing compute target {compute_name} so using it\")\n",
    "else:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_size,\n",
    "        vm_priority=vm_priority,\n",
    "        min_nodes=min_nodes,\n",
    "        max_nodes=max_nodes,\n",
    "    )\n",
    "\n",
    "    compute_target = ComputeTarget.create(aml_workspace, compute_name,\n",
    "                                                  compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our workspace and compute there. It's time to start creating a pipeline. It will be two steps in our pipeline: train a form recognizer model and preserve metadata information about the model in AML model store to make it available to scoring pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have two steps, we will need an entity to pass data from one step to another. We will use pipeline data. Every time when we run the pipeline, it will create an unique folder in our default blob and store our pipeline data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = PipelineData(\n",
    "    \"training_output\",\n",
    "    datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to execute training process that we implemented in train.py script. This script has several parameters like sas and form recognizer details, and it will save output inside out pipeline data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = PythonScriptStep(\n",
    "    name = \"training\",\n",
    "    script_name=\"train.py\",\n",
    "    inputs=[],\n",
    "    outputs=[training_output],\n",
    "    arguments=[\n",
    "        \"--sas_uri\", sas_uri, \n",
    "        \"--output\", training_output,\n",
    "        \"--fr_endpoint\", fr_endpoint,\n",
    "        \"--fr_key\", fr_key],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is taking output from the training step and register it in AML store. In fact, we could implement these two steps as a single step, but we wanted to show some aspects of AML (passing data between steps and multistep pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = PythonScriptStep(\n",
    "    name = \"registering\",\n",
    "    script_name=\"register.py\",\n",
    "    inputs=[training_output],\n",
    "    outputs=[],\n",
    "    arguments=[\"--input\", training_output],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a pipeline based on our two steps above. We just need to combine all the steps in an array and create Pipeline object using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [training_step, register_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=aml_workspace, steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to execute our pipeline. We use Experiment class to create a real execution and passing our pipeline as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(aml_workspace, 'train_basic_exp').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of success, we would like to preserve pipeline to execute it later using the Azure portal, Python SDK or Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.publish(\n",
    "    name=\"basic_training\",\n",
    "    description=\"Training form recognizer based on one data folder only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coursera)",
   "language": "python",
   "name": "coursera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
